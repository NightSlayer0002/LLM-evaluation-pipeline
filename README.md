# LLM-evaluation-pipeline

This repo contains an automated LLM Evaluation Pipeline designed to test AI model responses based on:

-Response Relevance & Completeness
-Hallucination / Factual Accuracy
-Latency & Cost Tracking

The pipeline runs in real-time, scales to millions of daily conversations, and supports any LLM provider (OpenAI, Claude, Grok, etc).
