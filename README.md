# LLM-evaluation-pipeline

This repo contains an automated LLM Evaluation Pipeline designed to test AI model responses based on:

- Response Relevance & Completeness
- Hallucination / Factual Accuracy
- Latency & Cost Tracking

The pipeline runs in real-time, scales to millions of daily conversations, and supports any LLM provider (OpenAI, Claude, Grok, etc).

## Local Setup
> git clone <repo-url>
> cd llm-eval-pipeline

> pip install -r requirements.txt

## Run the script:

> python main.py
